{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### comparing l1 regularized logit versus l2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, Y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000 35000 35000\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)\n",
    "print(len(X_train), len(Y_train), len(X_test))\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8335714285714285\n",
      "0.8283142857142857\n"
     ]
    }
   ],
   "source": [
    "logit = sklearn.linear_model.LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1000.0, fit_intercept=True, \n",
    "                                                intercept_scaling=1, class_weight=None, random_state=1998, \n",
    "                                                solver='liblinear', max_iter=100, multi_class='auto', \n",
    "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "logit.fit(X_train[:1000], Y_train[:1000])\n",
    "print(logit.score(X_train[0:1000], Y_train[0:1000]))\n",
    "print(logit.score(X_train, Y_train))\n",
    "print(logit.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8372\n",
      "0.8267142857142857\n"
     ]
    }
   ],
   "source": [
    "logit = sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1000.0, fit_intercept=True, \n",
    "                                                intercept_scaling=1, class_weight=None, random_state=1998, \n",
    "                                                solver='liblinear', max_iter=100, multi_class='auto', \n",
    "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "logit.fit(X_train[:1000], Y_train[:1000])\n",
    "print(logit.score(X_train[0:1000], Y_train[0:1000]))\n",
    "print(logit.score(X_train, Y_train))\n",
    "print(logit.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067643</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 0.0006, 'penalty': 'l1', 'solver': 'libl...</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060640</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 0.0005, 'penalty': 'l1', 'solver': 'libl...</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.057646</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 0.0004, 'penalty': 'l1', 'solver': 'libl...</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.010296</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.067643      0.004648         0.000593        0.000484  0.0006   \n",
       "1       0.060640      0.001327         0.000598        0.000489  0.0005   \n",
       "2       0.057646      0.004610         0.000399        0.000489  0.0004   \n",
       "\n",
       "  param_penalty param_solver  \\\n",
       "0            l1    liblinear   \n",
       "1            l1    liblinear   \n",
       "2            l1    liblinear   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'C': 0.0006, 'penalty': 'l1', 'solver': 'libl...              0.805   \n",
       "1  {'C': 0.0005, 'penalty': 'l1', 'solver': 'libl...              0.795   \n",
       "2  {'C': 0.0004, 'penalty': 'l1', 'solver': 'libl...              0.770   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0              0.800              0.790              0.805              0.765   \n",
       "1              0.805              0.780              0.790              0.770   \n",
       "2              0.760              0.745              0.775              0.765   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.793        0.015033                1  \n",
       "1            0.788        0.012083                2  \n",
       "2            0.763        0.010296                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859\n",
      "0.8220571428571428\n",
      "0.8169142857142857\n"
     ]
    }
   ],
   "source": [
    "logit = sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                                                intercept_scaling=1, class_weight=None, random_state=1998, \n",
    "                                                solver='lbfgs', max_iter=100, multi_class='auto', \n",
    "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "params = {\n",
    "            'solver': ['liblinear'],\n",
    "            'penalty': ['l1'],\n",
    "            'C': [6e-4, 5e-4, 4e-4]\n",
    "         }\n",
    "l1_logregcv = sklearn.model_selection.GridSearchCV(logit, params, scoring=None, n_jobs=None, \n",
    "                                                iid='deprecated', refit=True, cv=None, verbose=0, \n",
    "                                                pre_dispatch='2*n_jobs', return_train_score=False)\n",
    "\n",
    "l1_logregcv.fit(X_train[:1000], Y_train[:1000])\n",
    "display(pd.DataFrame(l1_logregcv.cv_results_).sort_values('rank_test_score').head())\n",
    "print(l1_logregcv.score(X_train[0:1000], Y_train[0:1000]))\n",
    "print(l1_logregcv.score(X_train, Y_train))\n",
    "print(l1_logregcv.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060741</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 1e-07, 'penalty': 'l2', 'solver': 'libli...</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050982</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>5e-08</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 5e-08, 'penalty': 'l2', 'solver': 'libli...</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.009695</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "1       0.060741      0.003600         0.000796        0.000398   1e-07   \n",
       "0       0.050982      0.002215         0.000598        0.000489   5e-08   \n",
       "\n",
       "  param_penalty param_solver  \\\n",
       "1            l2    liblinear   \n",
       "0            l2    liblinear   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "1  {'C': 1e-07, 'penalty': 'l2', 'solver': 'libli...              0.800   \n",
       "0  {'C': 5e-08, 'penalty': 'l2', 'solver': 'libli...              0.785   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "1               0.82              0.790              0.820               0.78   \n",
       "0               0.79              0.775              0.805               0.79   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "1            0.802        0.016000                1  \n",
       "0            0.789        0.009695                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "0.8226\n",
      "0.8192571428571429\n"
     ]
    }
   ],
   "source": [
    "logit = sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                                                intercept_scaling=1, class_weight=None, random_state=1998, \n",
    "                                                solver='lbfgs', max_iter=100, multi_class='auto', \n",
    "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "params = {\n",
    "            'solver': ['liblinear'],\n",
    "            'penalty': ['l2'],\n",
    "            'C': [ 5e-8, 1e-7]\n",
    "         }\n",
    "l2_logregcv = sklearn.model_selection.GridSearchCV(logit, params, scoring=None, n_jobs=None, \n",
    "                                                iid='deprecated', refit=True, cv=None, verbose=0, \n",
    "                                                pre_dispatch='2*n_jobs', return_train_score=False)\n",
    "\n",
    "l2_logregcv.fit(X_train[:1000], Y_train[:1000])\n",
    "display(pd.DataFrame(l2_logregcv.cv_results_).sort_values('rank_test_score').head())\n",
    "print(l2_logregcv.score(X_train[0:1000], Y_train[0:1000]))\n",
    "print(l2_logregcv.score(X_train, Y_train))\n",
    "print(l2_logregcv.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = 5000\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x784 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 411 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 185)\t0.000658796465853251\n",
      "  (0, 186)\t0.0001551847697164881\n",
      "  (0, 212)\t0.0002954378144834856\n",
      "  (0, 232)\t-0.0003115864280457127\n",
      "  (0, 351)\t-0.006399846471635558\n",
      "  (0, 353)\t-0.0010181112817155397\n",
      "  (0, 378)\t-0.0010024326205309306\n",
      "  (0, 379)\t-0.0017174052823570292\n",
      "  (0, 381)\t-0.00044408501053056393\n",
      "  (0, 386)\t0.00230102262927637\n",
      "  (0, 406)\t-0.0037793024305718416\n",
      "  (0, 408)\t-0.0010337584981581637\n",
      "  (0, 409)\t-0.00347223216864\n",
      "  (0, 434)\t-0.003068792323544469\n",
      "  (0, 436)\t-0.0026535225925181346\n",
      "  (0, 455)\t0.0009612894549161664\n",
      "  (0, 461)\t-0.001243958799584456\n",
      "  (0, 462)\t-0.00043714714494715897\n",
      "  (0, 464)\t-0.0033429268002897325\n",
      "  (0, 482)\t0.0006158399107614884\n",
      "  (0, 488)\t-8.480477619023793e-05\n",
      "  (0, 489)\t-0.0008099377429920558\n",
      "  (0, 490)\t-0.005871950060686973\n",
      "  (0, 491)\t-0.0012952356821026535\n",
      "  (0, 512)\t0.001667626848926752\n",
      "  :\t:\n",
      "  (9, 354)\t0.0011916009850971845\n",
      "  (9, 381)\t0.0009850104974022313\n",
      "  (9, 429)\t0.0008396181023440317\n",
      "  (9, 462)\t-0.0009351515700067087\n",
      "  (9, 467)\t-0.001325826376536821\n",
      "  (9, 490)\t-0.0004306739790056815\n",
      "  (9, 495)\t-0.002283516359539769\n",
      "  (9, 511)\t-0.000202473429646138\n",
      "  (9, 517)\t-0.00023939793195671433\n",
      "  (9, 544)\t-0.00013179241766342117\n",
      "  (9, 547)\t-0.00023709543119595553\n",
      "  (9, 549)\t-0.000540322278764403\n",
      "  (9, 550)\t-0.0005179346869823284\n",
      "  (9, 568)\t-1.9673789630593487e-06\n",
      "  (9, 569)\t-0.0016737358744353403\n",
      "  (9, 570)\t-0.00356402768787599\n",
      "  (9, 571)\t-0.0005332149105861184\n",
      "  (9, 572)\t-0.003095680959886172\n",
      "  (9, 574)\t-0.0011136159938387065\n",
      "  (9, 602)\t-0.0017397453677585315\n",
      "  (9, 603)\t-0.0005008177595992387\n",
      "  (9, 626)\t-0.002657119269406843\n",
      "  (9, 629)\t-0.0009742959161956757\n",
      "  (9, 656)\t-0.00014215346389496062\n",
      "  (9, 657)\t-0.0008815657580171129\n"
     ]
    }
   ],
   "source": [
    "chosen_l1 = l1_logregcv.best_estimator_\n",
    "chosen_l2 = l2_logregcv.best_estimator_\n",
    "chosen_l1.sparsify()\n",
    "chosen_l2.sparsify()\n",
    "display(chosen_l1.coef_)\n",
    "print(chosen_l1.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x784 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6120 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 40)\t-8.28305795923897e-07\n",
      "  (0, 41)\t-2.911155696215536e-06\n",
      "  (0, 42)\t-2.212994928054117e-06\n",
      "  (0, 61)\t-3.246283965760574e-07\n",
      "  (0, 62)\t-4.87734140338925e-07\n",
      "  (0, 63)\t-4.044933123302012e-07\n",
      "  (0, 64)\t-6.657181409427136e-07\n",
      "  (0, 66)\t-6.186081171335494e-07\n",
      "  (0, 67)\t-9.548989361500087e-06\n",
      "  (0, 68)\t-1.2704693749322872e-05\n",
      "  (0, 69)\t-6.294560530328271e-06\n",
      "  (0, 70)\t-2.3122815831768498e-05\n",
      "  (0, 71)\t-4.493123649307927e-05\n",
      "  (0, 72)\t-3.97119565118787e-05\n",
      "  (0, 73)\t-1.7912209891735256e-05\n",
      "  (0, 74)\t-1.6586769701371084e-05\n",
      "  (0, 75)\t-7.786550836087714e-06\n",
      "  (0, 76)\t-3.1712021388764506e-06\n",
      "  (0, 77)\t-5.710777864374245e-06\n",
      "  (0, 78)\t-9.506466701610341e-06\n",
      "  (0, 79)\t-6.624130156065581e-06\n",
      "  (0, 80)\t-4.081217631699173e-07\n",
      "  (0, 89)\t-3.279561013991467e-07\n",
      "  (0, 90)\t-7.249216971464635e-07\n",
      "  (0, 91)\t-1.0946206031200626e-06\n",
      "  :\t:\n",
      "  (9, 736)\t-1.2908286073690906e-05\n",
      "  (9, 737)\t2.035235528416246e-06\n",
      "  (9, 738)\t4.5749676504326406e-05\n",
      "  (9, 739)\t3.057374242897491e-05\n",
      "  (9, 740)\t9.873628512850926e-06\n",
      "  (9, 741)\t4.854911294639005e-05\n",
      "  (9, 742)\t3.2704074623536e-05\n",
      "  (9, 743)\t2.694178960977379e-05\n",
      "  (9, 744)\t6.236489767806087e-05\n",
      "  (9, 745)\t7.658212626940622e-05\n",
      "  (9, 746)\t2.887833104004574e-05\n",
      "  (9, 747)\t4.595368997006391e-05\n",
      "  (9, 748)\t5.072876501358577e-05\n",
      "  (9, 749)\t1.7106560681871975e-05\n",
      "  (9, 750)\t1.8675367936924472e-06\n",
      "  (9, 765)\t-1.515985323655234e-07\n",
      "  (9, 766)\t-2.472222220114689e-06\n",
      "  (9, 767)\t-2.5888364757804764e-06\n",
      "  (9, 768)\t-2.484371592328841e-06\n",
      "  (9, 769)\t1.347718596434767e-05\n",
      "  (9, 770)\t1.1692856295406287e-05\n",
      "  (9, 771)\t-3.862074851764132e-06\n",
      "  (9, 772)\t-7.992621277224347e-06\n",
      "  (9, 773)\t-4.361805999520041e-06\n",
      "  (9, 774)\t-1.6443316268031897e-06\n"
     ]
    }
   ],
   "source": [
    "display(chosen_l2.coef_) #need display to show the dataframe when using with in jupyter\n",
    "\n",
    "print(chosen_l2.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### experimenting with creating mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\utilites\\programming\\anaconda-python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1503: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8567142857142858\n",
      "0.8566285714285714\n"
     ]
    }
   ],
   "source": [
    "logit = sklearn.linear_model.LogisticRegression(penalty='none', dual=False, tol=0.0001, C=1000.0, fit_intercept=True, \n",
    "                                                intercept_scaling=1, class_weight=None, random_state=1998, \n",
    "                                                solver='newton-cg', max_iter=100, multi_class='auto', \n",
    "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "logit.fit(X_train[:1000], Y_train[:1000])\n",
    "print(logit.score(X_train[0:1000], Y_train[0:1000]))\n",
    "print(logit.score(X_train, Y_train))\n",
    "print(logit.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(logit.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x784 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6120 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 34)\t-5.845920736760732e-07\n",
      "  (0, 35)\t-8.358641053438595e-07\n",
      "  (0, 36)\t-7.978874179973279e-07\n",
      "  (0, 37)\t-5.672111530603455e-06\n",
      "  (0, 38)\t-1.583152893112945e-06\n",
      "  (0, 39)\t-8.382886572749282e-07\n",
      "  (0, 40)\t-2.423696242260286e-06\n",
      "  (0, 41)\t-1.588945369216028e-06\n",
      "  (0, 42)\t-2.164543420331665e-06\n",
      "  (0, 43)\t-2.954010452704373e-06\n",
      "  (0, 44)\t-3.895154346041973e-07\n",
      "  (0, 46)\t-5.648095392791253e-07\n",
      "  (0, 47)\t-1.2943551941813296e-06\n",
      "  (0, 48)\t-1.2590545979763837e-06\n",
      "  (0, 62)\t-2.410160303752233e-07\n",
      "  (0, 63)\t0.0011726474279174475\n",
      "  (0, 64)\t0.0002945406447052315\n",
      "  (0, 65)\t-8.140029775300432e-06\n",
      "  (0, 66)\t-5.991081404788263e-06\n",
      "  (0, 67)\t-1.0286553052798281e-05\n",
      "  (0, 68)\t-5.962445055867579e-05\n",
      "  (0, 69)\t-7.265027934625205e-05\n",
      "  (0, 70)\t-0.00010981981219144256\n",
      "  (0, 71)\t-0.0008231987870706076\n",
      "  (0, 72)\t-0.0015886854748027497\n",
      "  :\t:\n",
      "  (9, 721)\t0.003975065414860074\n",
      "  (9, 722)\t0.0010510025863958374\n",
      "  (9, 734)\t-2.5408012176432876e-06\n",
      "  (9, 735)\t-1.551226006561165e-05\n",
      "  (9, 736)\t0.007472854348537899\n",
      "  (9, 737)\t0.008555836195011891\n",
      "  (9, 738)\t0.005529169409689081\n",
      "  (9, 739)\t-0.0027155958609359244\n",
      "  (9, 740)\t-0.007238067551783738\n",
      "  (9, 741)\t0.0003369835399424864\n",
      "  (9, 742)\t-0.0019872482315836625\n",
      "  (9, 743)\t-0.0030657049921561353\n",
      "  (9, 744)\t0.0028651078393002716\n",
      "  (9, 745)\t0.008193812094950542\n",
      "  (9, 746)\t0.003018678707152669\n",
      "  (9, 747)\t0.0012960765280574525\n",
      "  (9, 748)\t0.0010618298599786294\n",
      "  (9, 766)\t-1.049205591886054e-06\n",
      "  (9, 767)\t-0.00012798676214501368\n",
      "  (9, 768)\t-0.0006749293071208956\n",
      "  (9, 769)\t-0.0005680576831649797\n",
      "  (9, 770)\t-5.49827208810404e-06\n",
      "  (9, 771)\t-2.4579616541269695e-07\n",
      "  (9, 772)\t8.044189203136305e-07\n",
      "  (9, 773)\t1.7093902056664626e-07\n"
     ]
    }
   ],
   "source": [
    "logit.sparsify()\n",
    "display(logit.coef_)\n",
    "print(logit.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.densify()\n",
    "len(logit.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.zeros(len(logit.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sums)):\n",
    "    for j in range(len(logit.coef_)):\n",
    "        sums[i] = sums[i] + abs(logit.coef_[j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.94376845e-03, 2.77924787e-03,\n",
       "       1.87068771e-04, 1.26740046e-03, 2.77863166e-04, 1.63534485e-04,\n",
       "       6.62584610e-04, 5.57574714e-04, 9.26383292e-04, 2.08364048e-03,\n",
       "       3.54850982e-04, 0.00000000e+00, 2.16354090e-04, 4.95811457e-04,\n",
       "       4.82289327e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 8.01378220e-04, 6.55858552e-03,\n",
       "       2.10610150e-03, 3.38965408e-03, 7.99854988e-03, 3.77464639e-03,\n",
       "       6.82217201e-03, 1.49474788e-02, 1.43177809e-02, 7.73288066e-03,\n",
       "       9.40095902e-03, 6.38582077e-03, 8.19953181e-03, 5.01265144e-03,\n",
       "       5.59802922e-04, 1.71888501e-04, 1.80697548e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 8.80817786e-04, 7.88214742e-03,\n",
       "       2.79513478e-03, 1.04148303e-02, 2.00783915e-02, 1.91316746e-02,\n",
       "       2.86253924e-02, 3.47687432e-02, 3.52344627e-02, 1.88928066e-02,\n",
       "       1.88313815e-02, 1.74249880e-02, 1.56792553e-02, 1.88484817e-02,\n",
       "       6.27972136e-03, 1.12195548e-02, 1.15208915e-02, 2.87422142e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.63748314e-05, 3.01915506e-05, 1.57945201e-03, 9.81738231e-03,\n",
       "       1.75872202e-02, 2.00979432e-02, 3.33744447e-02, 3.60604634e-02,\n",
       "       3.82875349e-02, 2.56242827e-02, 3.66175878e-02, 4.38330906e-02,\n",
       "       5.05616478e-02, 5.07610767e-02, 5.13622168e-02, 4.39853640e-02,\n",
       "       4.10003253e-02, 2.38761478e-02, 1.37591339e-02, 1.36389282e-02,\n",
       "       1.61628919e-03, 1.30524181e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       6.10874128e-04, 4.35047729e-03, 5.52308735e-03, 1.56081132e-02,\n",
       "       2.15324980e-02, 2.40013963e-02, 3.87456239e-02, 4.48344053e-02,\n",
       "       3.77674050e-02, 5.03195979e-02, 4.60455573e-02, 3.84241233e-02,\n",
       "       2.92916905e-02, 3.35997396e-02, 3.30483778e-02, 5.01481766e-02,\n",
       "       4.51333131e-02, 2.20584597e-02, 2.97527243e-02, 1.52691124e-02,\n",
       "       2.90574948e-03, 1.09343020e-03, 8.71812022e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 7.34201442e-05, 3.53035393e-04,\n",
       "       3.05891892e-03, 4.62233393e-03, 1.93721237e-02, 2.41613085e-02,\n",
       "       2.47457362e-02, 2.98832484e-02, 4.48329217e-02, 2.93553560e-02,\n",
       "       3.60401875e-02, 4.19465252e-02, 4.39471167e-02, 4.60211246e-02,\n",
       "       4.44449982e-02, 3.79581250e-02, 3.51946505e-02, 2.61603145e-02,\n",
       "       5.27098914e-02, 3.18827270e-02, 5.82281088e-02, 3.48217614e-02,\n",
       "       9.50558163e-03, 2.10864461e-03, 2.98119313e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.57519582e-04, 7.16334640e-04,\n",
       "       4.77352650e-03, 1.56157281e-02, 2.10445860e-02, 2.92998133e-02,\n",
       "       2.26523156e-02, 3.84503242e-02, 3.50152130e-02, 2.87870335e-02,\n",
       "       2.55273480e-02, 3.63497073e-02, 6.17476492e-02, 4.75114576e-02,\n",
       "       5.13965888e-02, 6.14345319e-02, 4.45929909e-02, 2.88452461e-02,\n",
       "       2.30420930e-02, 3.63557798e-02, 3.36487306e-02, 3.60566685e-02,\n",
       "       3.06432935e-02, 9.86158443e-03, 6.42542619e-03, 1.29217362e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.56930156e-05, 5.56766958e-03,\n",
       "       2.35350981e-02, 2.30641897e-02, 3.02186715e-02, 2.90737271e-02,\n",
       "       3.50707582e-02, 5.19081673e-02, 4.55708689e-02, 5.21630962e-02,\n",
       "       2.78782218e-02, 4.86155140e-02, 3.15506575e-02, 3.26434120e-02,\n",
       "       7.09366922e-02, 5.15046069e-02, 3.48870129e-02, 5.29325122e-02,\n",
       "       4.49833200e-02, 4.19938984e-02, 3.46007847e-02, 3.27293408e-02,\n",
       "       4.11949275e-02, 2.15883098e-02, 1.10369905e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.39252571e-05, 1.72335560e-02,\n",
       "       1.59298956e-02, 2.15538050e-02, 2.27416188e-02, 3.75563630e-02,\n",
       "       5.52346124e-02, 2.86619974e-02, 4.22429436e-02, 4.51536786e-02,\n",
       "       5.15471614e-02, 3.65095534e-02, 3.36910930e-02, 4.59978232e-02,\n",
       "       6.64598906e-02, 4.11247192e-02, 5.73475018e-02, 4.15477085e-02,\n",
       "       4.56752800e-02, 4.79129670e-02, 3.70398496e-02, 2.97567226e-02,\n",
       "       2.02156338e-02, 1.23422651e-02, 4.03485975e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.74974307e-04, 3.87169216e-03,\n",
       "       2.28153122e-02, 3.04862634e-02, 3.00203678e-02, 3.74803677e-02,\n",
       "       4.64770493e-02, 4.43537779e-02, 6.36409588e-02, 6.62436292e-02,\n",
       "       3.57633280e-02, 3.74196026e-02, 6.08671364e-02, 4.61579218e-02,\n",
       "       5.29291793e-02, 3.40485727e-02, 5.84966043e-02, 5.84587795e-02,\n",
       "       3.37660206e-02, 5.08907524e-02, 3.64499723e-02, 4.11967415e-02,\n",
       "       2.04522527e-02, 1.18985012e-02, 4.51867210e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.30210338e-03, 1.11479128e-02,\n",
       "       2.47199440e-02, 1.43271944e-02, 2.49002741e-02, 5.05681171e-02,\n",
       "       4.03303575e-02, 7.22517615e-02, 7.12614828e-02, 7.02705860e-02,\n",
       "       4.57889363e-02, 5.01000350e-02, 5.39042317e-02, 6.01124373e-02,\n",
       "       7.33371066e-02, 5.18627380e-02, 4.78136239e-02, 4.08093894e-02,\n",
       "       4.27570990e-02, 5.00726415e-02, 5.62627469e-02, 4.42053563e-02,\n",
       "       2.59332786e-02, 1.64011144e-02, 6.69912476e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.39967089e-03, 8.31539125e-03,\n",
       "       1.99767271e-02, 2.59350249e-02, 4.92532414e-02, 3.20485681e-02,\n",
       "       3.96916049e-02, 7.95044400e-02, 7.27156949e-02, 5.49372862e-02,\n",
       "       5.61061469e-02, 7.28349994e-02, 1.04649385e-01, 7.02004426e-02,\n",
       "       6.39035070e-02, 6.67916187e-02, 5.20528017e-02, 5.24050907e-02,\n",
       "       5.68176822e-02, 4.22307555e-02, 4.83450332e-02, 3.83090513e-02,\n",
       "       2.68876014e-02, 4.99162455e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.19431053e-04, 6.15985059e-03,\n",
       "       2.17140536e-02, 1.56853609e-02, 3.12502264e-02, 5.31050586e-02,\n",
       "       5.67530473e-02, 6.90089622e-02, 5.29560232e-02, 5.74401181e-02,\n",
       "       5.55679074e-02, 7.08855590e-02, 9.63632748e-02, 5.80546676e-02,\n",
       "       2.57609706e-02, 4.83153362e-02, 3.35323044e-02, 4.07673659e-02,\n",
       "       4.02169681e-02, 3.66942504e-02, 3.15380599e-02, 3.34499969e-02,\n",
       "       2.34673624e-02, 1.13030321e-02, 1.37883324e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.71290163e-06, 4.83380508e-03,\n",
       "       2.20244189e-02, 3.03590828e-02, 3.02411201e-02, 3.95673427e-02,\n",
       "       5.01993930e-02, 8.84543797e-02, 4.19593379e-02, 7.37458071e-02,\n",
       "       5.53746950e-02, 7.58554765e-02, 6.42267585e-02, 5.24937444e-02,\n",
       "       5.39237249e-02, 5.27215449e-02, 3.66039977e-02, 3.66919592e-02,\n",
       "       4.43226999e-02, 5.41814998e-02, 4.62680581e-02, 5.13478586e-02,\n",
       "       2.80150244e-02, 7.59943296e-03, 1.45140341e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.22498712e-04, 9.13084850e-04,\n",
       "       2.54399983e-02, 2.88162530e-02, 2.84140901e-02, 2.04698072e-02,\n",
       "       3.22508181e-02, 4.54689648e-02, 5.49807150e-02, 4.37332588e-02,\n",
       "       4.97396282e-02, 8.43753511e-02, 7.19170808e-02, 5.01726251e-02,\n",
       "       5.03395636e-02, 6.00500011e-02, 5.42849406e-02, 2.81708811e-02,\n",
       "       4.87896512e-02, 4.18274855e-02, 3.04852236e-02, 2.93493473e-02,\n",
       "       2.14730260e-02, 1.48890095e-03, 4.47025826e-05, 4.19086712e-06,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.04048443e-03, 1.23674455e-03,\n",
       "       1.92347703e-02, 2.87901822e-02, 2.92097901e-02, 3.01532076e-02,\n",
       "       4.28933179e-02, 4.64392115e-02, 4.56502410e-02, 5.74062475e-02,\n",
       "       5.54191496e-02, 5.90618494e-02, 8.34763466e-02, 5.04639379e-02,\n",
       "       4.57543868e-02, 6.99953804e-02, 4.65424407e-02, 4.45364990e-02,\n",
       "       3.14670825e-02, 5.81350994e-02, 4.64250465e-02, 3.22419688e-02,\n",
       "       9.32171737e-03, 8.64190052e-04, 1.59951428e-04, 7.68325638e-06,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.51943065e-03, 4.96303895e-03,\n",
       "       2.85609788e-02, 3.63817296e-02, 3.46876434e-02, 4.66049575e-02,\n",
       "       7.98435074e-02, 4.53192426e-02, 5.56354644e-02, 8.37362457e-02,\n",
       "       8.21406935e-02, 5.48702731e-02, 5.79336705e-02, 4.65883042e-02,\n",
       "       5.09925914e-02, 2.57699738e-02, 2.05928465e-02, 4.21771562e-02,\n",
       "       2.96213062e-02, 2.71146231e-02, 4.48267447e-02, 2.03717523e-02,\n",
       "       7.36178265e-03, 6.77742813e-04, 1.59951428e-04, 7.68325638e-06,\n",
       "       2.03019594e-06, 8.12078378e-06, 4.49029366e-04, 5.17745635e-03,\n",
       "       1.96410633e-02, 4.00931121e-02, 5.83199134e-02, 3.72493744e-02,\n",
       "       2.89531593e-02, 4.18114781e-02, 4.14419773e-02, 5.27639299e-02,\n",
       "       7.00130211e-02, 6.95960218e-02, 5.05799240e-02, 5.39772561e-02,\n",
       "       5.30254507e-02, 3.83880261e-02, 3.20457933e-02, 3.59063756e-02,\n",
       "       3.60462244e-02, 3.65737460e-02, 3.36520240e-02, 2.71244779e-02,\n",
       "       2.32378791e-02, 5.48327623e-03, 5.30843168e-05, 2.79391141e-06,\n",
       "       1.01509797e-05, 5.54920225e-05, 6.22490382e-04, 4.14337213e-03,\n",
       "       2.01790144e-02, 3.94792632e-02, 4.61220907e-02, 4.39711568e-02,\n",
       "       3.17110070e-02, 5.09633450e-02, 4.56607223e-02, 4.99556508e-02,\n",
       "       5.42542837e-02, 5.94572827e-02, 4.55294420e-02, 2.84706374e-02,\n",
       "       2.81308340e-02, 4.58920694e-02, 4.27071863e-02, 3.00336994e-02,\n",
       "       3.36411545e-02, 3.90171973e-02, 3.96594073e-02, 2.36474754e-02,\n",
       "       2.14452209e-02, 5.33044047e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.26089465e-04, 4.71810517e-03,\n",
       "       2.71509817e-02, 4.89660333e-02, 3.86476124e-02, 4.66703169e-02,\n",
       "       5.51415227e-02, 4.42992843e-02, 5.98791423e-02, 6.08605923e-02,\n",
       "       4.31765591e-02, 3.16463465e-02, 3.85320996e-02, 2.85857577e-02,\n",
       "       4.95427572e-02, 3.54532925e-02, 4.64809993e-02, 3.82450084e-02,\n",
       "       2.44718717e-02, 3.21874482e-02, 2.63423791e-02, 1.58813164e-02,\n",
       "       1.41261343e-02, 1.37631011e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 4.60788974e-06, 4.20757071e-03,\n",
       "       1.35537066e-02, 2.08410883e-02, 3.16313521e-02, 5.44345467e-02,\n",
       "       5.54591165e-02, 6.57066649e-02, 4.44807090e-02, 6.54838005e-02,\n",
       "       3.99574278e-02, 2.49171842e-02, 2.27742694e-02, 2.36706527e-02,\n",
       "       2.95968413e-02, 3.19665339e-02, 3.08520319e-02, 3.86869274e-02,\n",
       "       2.32919052e-02, 1.96967180e-02, 3.35357119e-02, 2.02140204e-02,\n",
       "       2.94405819e-03, 8.25754408e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.60112431e-03, 2.71014947e-03,\n",
       "       1.81220789e-02, 2.92704643e-02, 2.87156098e-02, 2.43047849e-02,\n",
       "       3.70601144e-02, 3.06621233e-02, 3.77901618e-02, 4.66217684e-02,\n",
       "       3.44741205e-02, 4.80041899e-02, 2.33349126e-02, 4.92215022e-02,\n",
       "       3.03762806e-02, 2.71953396e-02, 3.63019697e-02, 4.13996971e-02,\n",
       "       3.45671965e-02, 2.70946700e-02, 2.29876978e-02, 1.12691528e-02,\n",
       "       1.19848048e-03, 9.15524429e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.97204427e-03, 3.47033275e-03,\n",
       "       1.34104351e-02, 2.86288514e-02, 2.53006651e-02, 2.49040588e-02,\n",
       "       4.81759682e-02, 2.92819944e-02, 2.58352385e-02, 2.34725366e-02,\n",
       "       5.90941068e-02, 6.86978095e-02, 6.25579957e-02, 5.41534987e-02,\n",
       "       4.86683663e-02, 6.13878492e-02, 2.89883242e-02, 3.19063148e-02,\n",
       "       2.67165622e-02, 2.39499353e-02, 1.48047684e-02, 1.02416516e-02,\n",
       "       7.23315508e-03, 1.45654257e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.13820947e-03,\n",
       "       9.33001279e-03, 1.29430477e-02, 2.63992733e-02, 2.60838209e-02,\n",
       "       3.48872179e-02, 3.47266738e-02, 4.23577908e-02, 2.70671985e-02,\n",
       "       4.40814809e-02, 4.77378875e-02, 4.83353678e-02, 5.01112286e-02,\n",
       "       1.74015241e-02, 3.36269730e-02, 1.88029997e-02, 3.10217357e-02,\n",
       "       2.15250332e-02, 1.30211018e-02, 6.22657485e-03, 4.94831565e-03,\n",
       "       3.06987275e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.35289200e-03,\n",
       "       8.92928080e-03, 8.87869864e-03, 2.62299850e-02, 2.44906753e-02,\n",
       "       2.02964358e-02, 3.74710118e-02, 2.92022284e-02, 2.22163857e-02,\n",
       "       3.62866023e-02, 3.75094625e-02, 3.64820443e-02, 4.12532502e-02,\n",
       "       4.32403228e-02, 3.94544401e-02, 2.61235281e-02, 1.28689511e-02,\n",
       "       1.43100636e-02, 8.05386795e-03, 2.10200517e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 7.48906305e-06, 4.57227007e-05,\n",
       "       1.53473265e-02, 2.64062133e-02, 1.71580240e-02, 1.42708739e-02,\n",
       "       1.93897009e-02, 1.50277467e-02, 1.14947192e-02, 1.20294416e-02,\n",
       "       7.73672221e-03, 1.63876242e-02, 6.25972736e-03, 2.95164101e-03,\n",
       "       2.12496049e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.45234015e-05, 6.69927796e-04,\n",
       "       1.96105788e-03, 1.61135100e-03, 9.47682387e-05, 1.88200354e-05,\n",
       "       1.85662239e-05, 3.94532257e-06, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([350, 378, 401, 433, 487, 462, 488, 484, 345, 405, 403, 324, 349,\n",
       "       346, 317, 434, 318, 240, 377, 319, 351, 516, 465, 517, 373, 657,\n",
       "       353, 268, 291, 597, 599, 406, 352, 290, 658, 210, 213, 661, 294,\n",
       "       571, 323, 437, 570, 545, 656, 461, 298, 299, 510, 190, 469, 379,\n",
       "       490, 375, 459, 270, 356, 372, 330, 348, 486, 376, 596, 460, 404,\n",
       "       260, 568, 430, 347, 489, 595, 438, 544, 413, 659, 519, 408, 322,\n",
       "       371, 520, 374, 243, 296, 515, 409, 188, 407, 355, 235, 354, 233,\n",
       "       325, 264, 241, 212, 130, 415, 492, 541, 301, 129, 518, 315, 128,\n",
       "       463, 436, 153, 400, 435, 159, 687, 321, 329, 543, 432, 576, 342,\n",
       "       631, 565, 440, 660, 237, 358, 686, 381, 652, 629, 273, 326, 685,\n",
       "       211, 567, 627, 483, 491, 466, 578, 288, 457, 470, 414, 295, 538,\n",
       "       154, 183, 267, 549, 320, 464, 272, 542, 458, 234, 546, 429, 485,\n",
       "       263, 160, 244, 151, 178, 498, 214, 467, 598, 184, 289, 412, 569,\n",
       "       331, 684, 131, 539, 182, 127, 431, 716, 572, 456, 328, 550, 682,\n",
       "       262, 357, 495, 245, 402, 181, 441, 513, 271, 514, 635, 715, 303,\n",
       "       248, 269, 132, 327, 383, 316, 384, 509, 600, 344, 554, 399, 537,\n",
       "       717, 553, 150, 607, 566, 574, 205, 155, 521, 359, 124, 579, 185,\n",
       "       626, 152, 259, 713, 287, 709, 293, 511, 624, 274, 385, 411, 126,\n",
       "       410, 525, 265, 714, 302, 481, 217, 209, 634, 712, 123, 219, 524,\n",
       "       180, 523, 292, 577,  98, 186, 232, 206, 680, 242, 191,  97, 681,\n",
       "       482, 246, 636, 628, 297, 300, 266, 526, 218, 552, 689, 157, 610,\n",
       "       382, 387, 122, 158, 247, 239, 428, 471, 581, 343, 522, 605, 663,\n",
       "       189, 540, 573, 594, 238, 386, 468, 370, 691, 606, 625, 220, 285,\n",
       "       442, 632, 397, 398, 230, 455, 551, 286, 177, 275, 162, 496, 604,\n",
       "       179, 443, 203, 156, 653, 621, 454, 710, 231, 662, 512, 215, 425,\n",
       "       453, 207, 622, 261, 649,  96, 575, 480, 547, 426, 439, 548, 416,\n",
       "       236, 633, 564, 527, 497, 637, 683, 360, 664, 737, 678, 582, 706,\n",
       "       187, 718, 679, 341, 332, 654, 493, 380, 125, 208, 424, 650, 601,\n",
       "       651, 314, 176, 312, 707, 580, 623, 175, 149, 665, 133, 603, 555,\n",
       "       228, 655, 388, 630, 608, 528, 229, 216, 638, 284, 602, 258, 204,\n",
       "       711, 161, 396, 368, 249, 257, 148, 692, 444, 556, 202, 593, 494,\n",
       "       427, 304, 499, 708, 276, 611, 536, 121,  94, 340, 609, 508, 740,\n",
       "       174, 452,  95,  99, 103, 100, 690, 620, 120, 101, 688, 255, 738,\n",
       "       333, 745, 256, 583, 369, 102, 201, 147, 736, 163, 741,  69, 666,\n",
       "       313,  70, 720, 739, 584, 390, 134, 135, 592, 648, 693, 677, 719,\n",
       "       277, 743, 305, 106, 742, 389, 639, 105, 311, 250,  93, 667, 221,\n",
       "       119, 192,  72, 676, 472, 704, 705, 339,  74, 721,  66,  91, 744,\n",
       "        71, 417, 500, 668,  68, 334,  63, 222,  73, 104, 746, 694, 367,\n",
       "       227, 146, 529, 557, 507,  75, 479, 695, 395, 200, 563, 173, 306,\n",
       "       145, 310, 591, 535, 278, 283,  67, 618, 647,  65, 703, 696, 172,\n",
       "       747, 612, 164, 107,  92,  35, 619, 748, 193,  64, 722,  43, 646,\n",
       "       768,  34, 136, 769, 118, 478, 445, 669, 418, 338, 585, 223,  37,\n",
       "       451, 640, 675, 165, 450,  42, 641, 423,  90, 473, 613,  62, 199,\n",
       "       501, 767,  40, 534, 144,  76,  41, 361,  47,  48, 506, 422, 282,\n",
       "        44, 171, 366, 194,  38,  46,  36,  77,  39, 502, 474, 198, 137,\n",
       "       562, 770, 166, 170, 533, 530, 735, 446, 766, 117, 226, 771, 772,\n",
       "        78, 116, 254, 532, 505, 503, 475, 734, 394, 590, 447, 773, 531,\n",
       "       504, 726, 724, 725, 733, 728, 729, 730, 731, 732, 779, 780, 781,\n",
       "       727, 749, 757, 751, 774, 672, 673, 674, 671, 670, 702, 765, 764,\n",
       "       763, 775, 776, 777, 778, 750, 701, 700, 699, 761, 723, 760, 759,\n",
       "       758, 756, 755, 754, 698, 697, 753, 752, 762,   0, 391, 644,  32,\n",
       "        33,  45,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,\n",
       "        60,  61,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
       "        31, 108,  30,  28,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
       "        10,  11,  12,  13,  14,  15,  16,  17,  18,  19,  20,  21,  22,\n",
       "        23,  24,  25,  26,  27,  29, 645, 109, 111, 363, 364, 365, 782,\n",
       "       392, 393, 419, 420, 421, 448, 449, 476, 477, 558, 559, 560, 561,\n",
       "       586, 587, 588, 589, 614, 615, 616, 617, 642, 643, 362, 110, 337,\n",
       "       335, 112, 113, 114, 115, 138, 139, 140, 141, 142, 143, 167, 168,\n",
       "       169, 195, 196, 197, 224, 225, 251, 252, 253, 279, 280, 281, 307,\n",
       "       308, 309, 336, 783], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_sums = np.argsort(-abs(sums))\n",
    "sorted_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([350, 378, 401, 433, 487, 462, 488, 484, 345, 405, 403, 324, 349,\n",
       "       346, 317, 434, 318, 240, 377, 319, 351, 516, 465, 517, 373, 657,\n",
       "       353, 268, 291, 597, 599, 406, 352, 290, 658, 210, 213, 661, 294,\n",
       "       571, 323, 437, 570, 545, 656, 461, 298, 299, 510, 190, 469, 379,\n",
       "       490, 375, 459, 270, 356, 372, 330, 348, 486, 376, 596, 460, 404,\n",
       "       260, 568, 430, 347, 489, 595, 438, 544, 413, 659, 519, 408, 322,\n",
       "       371, 520], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_indices = sorted_sums[0:80:1]\n",
    "useful_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hardcut = X[:, useful_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_hardcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_hardcut[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000 35000 35000\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_hardcut, Y, test_size=0.5, random_state=1998)\n",
    "print(len(X_train), len(Y_train), len(X_test))\n",
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\utilites\\programming\\anaconda-python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1503: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7282857142857143\n",
      "0.7189714285714286\n"
     ]
    }
   ],
   "source": [
    "logit = sklearn.linear_model.LogisticRegression(penalty='none', dual=False, tol=0.0001, C=1000.0, fit_intercept=True, \n",
    "                                                intercept_scaling=1, class_weight=None, random_state=1998, \n",
    "                                                solver='newton-cg', max_iter=100, multi_class='auto', \n",
    "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "logit.fit(X_train[:1000], Y_train[:1000])\n",
    "print(logit.score(X_train[0:1000], Y_train[0:1000]))\n",
    "print(logit.score(X_train, Y_train))\n",
    "print(logit.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020944</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 1e-07, 'penalty': 'l2', 'solver': 'libli...</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.011576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095194</td>\n",
       "      <td>0.155990</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>5e-08</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 5e-08, 'penalty': 'l2', 'solver': 'libli...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "1       0.020944      0.002821         0.000798        0.000399   1e-07   \n",
       "0       0.095194      0.155990         0.000798        0.000399   5e-08   \n",
       "\n",
       "  param_penalty param_solver  \\\n",
       "1            l2    liblinear   \n",
       "0            l2    liblinear   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "1  {'C': 1e-07, 'penalty': 'l2', 'solver': 'libli...               0.68   \n",
       "0  {'C': 5e-08, 'penalty': 'l2', 'solver': 'libli...               0.64   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "1               0.69               0.71              0.705              0.685   \n",
       "0               0.66               0.68              0.660              0.655   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "1            0.694        0.011576                1  \n",
       "0            0.659        0.012806                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721\n",
      "0.6832\n",
      "0.6809142857142857\n"
     ]
    }
   ],
   "source": [
    "logit = sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                                                intercept_scaling=1, class_weight=None, random_state=1998, \n",
    "                                                solver='lbfgs', max_iter=100, multi_class='auto', \n",
    "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "params = {\n",
    "            'solver': ['liblinear'],\n",
    "            'penalty': ['l2'],\n",
    "            'C': [ 5e-8, 1e-7]\n",
    "         }\n",
    "l2_logregcv = sklearn.model_selection.GridSearchCV(logit, params, scoring=None, n_jobs=None, \n",
    "                                                iid='deprecated', refit=True, cv=None, verbose=0, \n",
    "                                                pre_dispatch='2*n_jobs', return_train_score=False)\n",
    "\n",
    "l2_logregcv.fit(X_train[:1000], Y_train[:1000])\n",
    "display(pd.DataFrame(l2_logregcv.cv_results_).sort_values('rank_test_score').head())\n",
    "print(l2_logregcv.score(X_train[0:1000], Y_train[0:1000]))\n",
    "print(l2_logregcv.score(X_train, Y_train))\n",
    "print(l2_logregcv.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 0.0006, 'penalty': 'l1', 'solver': 'libl...</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.014629</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021343</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 0.0005, 'penalty': 'l1', 'solver': 'libl...</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.003369</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'C': 0.0004, 'penalty': 'l1', 'solver': 'libl...</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.015937</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.023392      0.001832         0.000199        0.000397  0.0006   \n",
       "1       0.021343      0.000798         0.000399        0.000489  0.0005   \n",
       "2       0.021349      0.003369         0.000399        0.000488  0.0004   \n",
       "\n",
       "  param_penalty param_solver  \\\n",
       "0            l1    liblinear   \n",
       "1            l1    liblinear   \n",
       "2            l1    liblinear   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'C': 0.0006, 'penalty': 'l1', 'solver': 'libl...              0.800   \n",
       "1  {'C': 0.0005, 'penalty': 'l1', 'solver': 'libl...              0.780   \n",
       "2  {'C': 0.0004, 'penalty': 'l1', 'solver': 'libl...              0.765   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0              0.795              0.775               0.76              0.775   \n",
       "1              0.770              0.755               0.75              0.780   \n",
       "2              0.755              0.745               0.72              0.760   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.781        0.014629                1  \n",
       "1            0.767        0.012490                2  \n",
       "2            0.749        0.015937                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825\n",
      "0.7757142857142857\n",
      "0.7707142857142857\n"
     ]
    }
   ],
   "source": [
    "logit = sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, \n",
    "                                                intercept_scaling=1, class_weight=None, random_state=1998, \n",
    "                                                solver='lbfgs', max_iter=100, multi_class='auto', \n",
    "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "params = {\n",
    "            'solver': ['liblinear'],\n",
    "            'penalty': ['l1'],\n",
    "            'C': [6e-4, 5e-4, 4e-4]\n",
    "         }\n",
    "l1_logregcv = sklearn.model_selection.GridSearchCV(logit, params, scoring=None, n_jobs=None, \n",
    "                                                iid='deprecated', refit=True, cv=None, verbose=0, \n",
    "                                                pre_dispatch='2*n_jobs', return_train_score=False)\n",
    "\n",
    "l1_logregcv.fit(X_train[:1000], Y_train[:1000])\n",
    "display(pd.DataFrame(l1_logregcv.cv_results_).sort_values('rank_test_score').head())\n",
    "print(l1_logregcv.score(X_train[0:1000], Y_train[0:1000]))\n",
    "print(l1_logregcv.score(X_train, Y_train))\n",
    "print(l1_logregcv.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
